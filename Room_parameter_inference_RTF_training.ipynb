{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29757e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# Graphic output\n",
    "from IPython.display import SVG\n",
    "from keras.models import load_model\n",
    "from IPython.display import Image\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bebc7",
   "metadata": {},
   "source": [
    "# GPU status detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d0924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46b0d9",
   "metadata": {},
   "source": [
    "# Auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa526b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PlotProgress subclasses the Callback class to plot graph after each epoch\n",
    "class PlotProgress(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, entity='loss'):\n",
    "        self.entity = entity\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('{}'.format(self.entity)))\n",
    "        self.val_losses.append(logs.get('val_{}'.format(self.entity)))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"{}\".format(self.entity))\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_{}\".format(self.entity))\n",
    "        plt.legend()\n",
    "        plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a837c",
   "metadata": {},
   "source": [
    "# Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d08805",
   "metadata": {
    "code_folding": [
     9,
     16,
     41
    ]
   },
   "outputs": [],
   "source": [
    "# data_loader\n",
    "# -input:  csv files path\n",
    "# -output: absorption coeffcients for each surfaces in 63Hz,125Hz and 250Hz, 500Hz octave band // room size // Transfer functions\n",
    "class DataLoader(object):\n",
    "    def __init__(self,path,ratio):\n",
    "        self.path = path\n",
    "        self.ratio = ratio\n",
    "        self.data = []\n",
    "    # dataset length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    #  return one data\n",
    "    def __get__(self,idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def load_data(self):\n",
    "        # Get all csvs in your dir.\n",
    "        files = Path(path).glob('*.csv')\n",
    "        df_list = []\n",
    "\n",
    "        # Put all the csv file togther\n",
    "        for file in files:\n",
    "            df_list.append(pd.read_csv(file))\n",
    "        self.data = pd.concat(df_list)\n",
    "        # Data perturbation by row, keep the order\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        # Split absorption coeffcient, room size and transfer functions\n",
    "        dfs = np.split(self.data, [4,8,12,16,20,24,27], axis=1) \n",
    "        self.ac1 = dfs[0]\n",
    "        self.ac2 = dfs[1]\n",
    "        self.ac3 = dfs[2]\n",
    "        self.ac4 = dfs[3]\n",
    "        self.ac5 = dfs[4]\n",
    "        self.ac6 = dfs[5]\n",
    "        self.RoomGeo = dfs[6]\n",
    "        self.TransFun = dfs[7]\n",
    "        \n",
    "        return self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        # Testing to training rario\n",
    "        Ratio = self.ratio\n",
    "\n",
    "        ## Lable split\n",
    "        ac1_63 = self.ac1['1-63']\n",
    "        ac1_125 = self.ac1['1-125']\n",
    "        ac1_250 = self.ac1['1-250']\n",
    "        ac1_500 = self.ac1['1-500']\n",
    "\n",
    "        ac2_63 = self.ac2['2-63']\n",
    "        ac2_125 = self.ac2['2-125']\n",
    "        ac2_250 = self.ac2['2-250']\n",
    "        ac2_500 = self.ac2['2-500']\n",
    "\n",
    "        ac3_63 = self.ac3['3-63']\n",
    "        ac3_125 = self.ac3['3-125']\n",
    "        ac3_250 = self.ac3['3-250']\n",
    "        ac3_500 = self.ac3['3-500']\n",
    "\n",
    "        ac4_63 = self.ac4['4-63']\n",
    "        ac4_125 = self.ac4['4-125']\n",
    "        ac4_250 = self.ac4['4-250']\n",
    "        ac4_500 = self.ac4['4-500']\n",
    "\n",
    "        ac5_63 = self.ac5['5-63']\n",
    "        ac5_125 = self.ac5['5-125']\n",
    "        ac5_250 = self.ac5['5-250']\n",
    "        ac5_500 = self.ac5['5-500']\n",
    "\n",
    "        ac6_63 = self.ac6['6-63']\n",
    "        ac6_125 = self.ac6['6-125']\n",
    "        ac6_250 = self.ac6['6-250']\n",
    "        ac6_500 = self.ac6['6-500']\n",
    "\n",
    "        # 63Hz dataset label\n",
    "        [y_train_ac1_63,y_test_ac1_63] = np.split(ac1_63, [int(ac1_63.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac2_63,y_test_ac2_63] = np.split(ac2_63, [int(ac2_63.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac3_63,y_test_ac3_63] = np.split(ac3_63, [int(ac3_63.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac4_63,y_test_ac4_63] = np.split(ac4_63, [int(ac4_63.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac5_63,y_test_ac5_63] = np.split(ac5_63, [int(ac5_63.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac6_63,y_test_ac6_63] = np.split(ac6_63, [int(ac6_63.shape[0]*Ratio)], axis=0)\n",
    "        y_train_ac1_63 =  y_train_ac1_63.values.astype(\"float32\")\n",
    "        y_test_ac1_63 =  y_test_ac1_63.values.astype(\"float32\")\n",
    "        y_train_ac2_63 =  y_train_ac2_63.values.astype(\"float32\")\n",
    "        y_test_ac2_63 =  y_test_ac2_63.values.astype(\"float32\")\n",
    "        y_train_ac3_63 =  y_train_ac3_63.values.astype(\"float32\")\n",
    "        y_test_ac3_63 =  y_test_ac3_63.values.astype(\"float32\")\n",
    "        y_train_ac4_63 =  y_train_ac4_63.values.astype(\"float32\")\n",
    "        y_test_ac4_63 =  y_test_ac4_63.values.astype(\"float32\")\n",
    "        y_train_ac5_63 =  y_train_ac5_63.values.astype(\"float32\")\n",
    "        y_test_ac5_63 =  y_test_ac5_63.values.astype(\"float32\")\n",
    "        y_train_ac6_63 =  y_train_ac6_63.values.astype(\"float32\")\n",
    "        y_test_ac6_63 =  y_test_ac6_63.values.astype(\"float32\")\n",
    "\n",
    "\n",
    "        # 125Hz dataset label\n",
    "        [y_train_ac1_125,y_test_ac1_125] = np.split(ac1_125, [int(ac1_125.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac2_125,y_test_ac2_125] = np.split(ac2_125, [int(ac2_125.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac3_125,y_test_ac3_125] = np.split(ac3_125, [int(ac3_125.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac4_125,y_test_ac4_125] = np.split(ac4_125, [int(ac4_125.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac5_125,y_test_ac5_125] = np.split(ac5_125, [int(ac5_125.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac6_125,y_test_ac6_125] = np.split(ac6_125, [int(ac6_125.shape[0]*Ratio)], axis=0)\n",
    "        y_train_ac1_125 =  y_train_ac1_125.values.astype(\"float32\")\n",
    "        y_test_ac1_125 =  y_test_ac1_125.values.astype(\"float32\")\n",
    "        y_train_ac2_125 =  y_train_ac2_125.values.astype(\"float32\")\n",
    "        y_test_ac2_125 =  y_test_ac2_125.values.astype(\"float32\")\n",
    "        y_train_ac3_125 =  y_train_ac3_125.values.astype(\"float32\")\n",
    "        y_test_ac3_125 =  y_test_ac3_125.values.astype(\"float32\")\n",
    "        y_train_ac4_125 =  y_train_ac4_125.values.astype(\"float32\")\n",
    "        y_test_ac4_125 =  y_test_ac4_125.values.astype(\"float32\")\n",
    "        y_train_ac5_125 =  y_train_ac5_125.values.astype(\"float32\")\n",
    "        y_test_ac5_125 =  y_test_ac5_125.values.astype(\"float32\")\n",
    "        y_train_ac6_125 =  y_train_ac6_125.values.astype(\"float32\")\n",
    "        y_test_ac6_125 =  y_test_ac6_125.values.astype(\"float32\")\n",
    "\n",
    "        # 250Hz dataset label\n",
    "        [y_train_ac1_250,y_test_ac1_250] = np.split(ac1_250, [int(ac1_250.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac2_250,y_test_ac2_250] = np.split(ac2_250, [int(ac2_250.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac3_250,y_test_ac3_250] = np.split(ac3_250, [int(ac3_250.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac4_250,y_test_ac4_250] = np.split(ac4_250, [int(ac4_250.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac5_250,y_test_ac5_250] = np.split(ac5_250, [int(ac5_250.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac6_250,y_test_ac6_250] = np.split(ac6_250, [int(ac6_250.shape[0]*Ratio)], axis=0)\n",
    "        y_train_ac1_250 =  y_train_ac1_250.values.astype(\"float32\")\n",
    "        y_test_ac1_250 =  y_test_ac1_250.values.astype(\"float32\")\n",
    "        y_train_ac2_250 =  y_train_ac2_250.values.astype(\"float32\")\n",
    "        y_test_ac2_250 =  y_test_ac2_250.values.astype(\"float32\")\n",
    "        y_train_ac3_250 =  y_train_ac3_250.values.astype(\"float32\")\n",
    "        y_test_ac3_250 =  y_test_ac3_250.values.astype(\"float32\")\n",
    "        y_train_ac4_250 =  y_train_ac4_250.values.astype(\"float32\")\n",
    "        y_test_ac4_250 =  y_test_ac4_250.values.astype(\"float32\")\n",
    "        y_train_ac5_250 =  y_train_ac5_250.values.astype(\"float32\")\n",
    "        y_test_ac5_250 =  y_test_ac5_250.values.astype(\"float32\")\n",
    "        y_train_ac6_250 =  y_train_ac6_250.values.astype(\"float32\")\n",
    "        y_test_ac6_250 =  y_test_ac6_250.values.astype(\"float32\")\n",
    "\n",
    "        # 500Hz dataset label\n",
    "        [y_train_ac1_500,y_test_ac1_500] = np.split(ac1_500, [int(ac1_500.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac2_500,y_test_ac2_500] = np.split(ac2_500, [int(ac2_500.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac3_500,y_test_ac3_500] = np.split(ac3_500, [int(ac3_500.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac4_500,y_test_ac4_500] = np.split(ac4_500, [int(ac4_500.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac5_500,y_test_ac5_500] = np.split(ac5_500, [int(ac5_500.shape[0]*Ratio)], axis=0)\n",
    "        [y_train_ac6_500,y_test_ac6_500] = np.split(ac6_500, [int(ac6_500.shape[0]*Ratio)], axis=0)\n",
    "        y_train_ac1_500 =  y_train_ac1_500.values.astype(\"float32\")\n",
    "        y_test_ac1_500 =  y_test_ac1_500.values.astype(\"float32\")\n",
    "        y_train_ac2_500 =  y_train_ac2_500.values.astype(\"float32\")\n",
    "        y_test_ac2_500 =  y_test_ac2_500.values.astype(\"float32\")\n",
    "        y_train_ac3_500 =  y_train_ac3_500.values.astype(\"float32\")\n",
    "        y_test_ac3_500 =  y_test_ac3_500.values.astype(\"float32\")\n",
    "        y_train_ac4_500 =  y_train_ac4_500.values.astype(\"float32\")\n",
    "        y_test_ac4_500 =  y_test_ac4_500.values.astype(\"float32\")\n",
    "        y_train_ac5_500 =  y_train_ac5_500.values.astype(\"float32\")\n",
    "        y_test_ac5_500 =  y_test_ac5_500.values.astype(\"float32\")\n",
    "        y_train_ac6_500 =  y_train_ac6_500.values.astype(\"float32\")\n",
    "        y_test_ac6_500 =  y_test_ac6_500.values.astype(\"float32\")\n",
    "\n",
    "        # Room size\n",
    "        [y_train_roomsize,y_test_roomsize] = np.split(self.RoomGeo, [int(self.RoomGeo.shape[0]*Ratio)], axis=0)\n",
    "        y_train_roomsize =  y_train_roomsize.values.astype(\"float32\")\n",
    "        y_test_roomsize =  y_test_roomsize.values.astype(\"float32\")\n",
    "\n",
    "        ## Transfer function \n",
    "        [x_train,x_test] = np.split(self.TransFun, [int(self.TransFun.shape[0]*Ratio)], axis=0)\n",
    "        x_train =  x_train.values\n",
    "        x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "        x_test =  x_test.values\n",
    "        x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
    "        x_train = x_train.astype(\"float32\")\n",
    "        x_test = x_test.astype(\"float32\")\n",
    "        \n",
    "        return x_train, x_test,y_train_ac1_63, y_train_ac1_125, y_train_ac1_250, y_train_ac1_500,\\\n",
    "                y_train_ac2_63, y_train_ac2_125, y_train_ac2_250, y_train_ac2_500,\\\n",
    "                    y_train_ac3_63, y_train_ac3_125, y_train_ac3_250, y_train_ac3_500,\\\n",
    "                y_train_ac4_63, y_train_ac4_125, y_train_ac4_250, y_train_ac4_500,\\\n",
    "                    y_train_ac5_63, y_train_ac5_125, y_train_ac5_250, y_train_ac5_500,\\\n",
    "                y_train_ac6_63, y_train_ac6_125, y_train_ac6_250, y_train_ac6_500,\\\n",
    "                    y_test_ac1_63, y_test_ac1_125, y_test_ac1_250, y_test_ac1_500,\\\n",
    "                y_test_ac2_63, y_test_ac2_125, y_test_ac2_250, y_test_ac2_500,\\\n",
    "                    y_test_ac3_63, y_test_ac3_125, y_test_ac3_250, y_test_ac3_500,\\\n",
    "                    y_test_ac4_63, y_test_ac4_125, y_test_ac4_250, y_test_ac4_500,\\\n",
    "                    y_test_ac5_63, y_test_ac5_125, y_test_ac5_250, y_test_ac5_500,\\\n",
    "                    y_test_ac6_63, y_test_ac6_125, y_test_ac6_250, y_test_ac6_500,y_train_roomsize, y_test_roomsize  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc4a6e",
   "metadata": {},
   "source": [
    "# Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0bf60",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ResNet(object):\n",
    "    def __init__(self, length, num_channel, num_filters, room_size,pooling='avg', dropout_rate=False):\n",
    "        self.length = length\n",
    "        self.num_channel = num_channel\n",
    "        self.num_filters = num_filters\n",
    "        self.pooling = pooling\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.room_size = room_size\n",
    "    # Neural network building blocks\n",
    "    def Conv_1D_Block(self,x, num_filters, kernel, strides):\n",
    "        # 1D Convolutional Block with BatchNormalization\n",
    "        x = tf.keras.layers.Conv1D(num_filters, kernel, strides=strides, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def conv_block(self,inputs, num_filters):\n",
    "        # Construct Block of Convolutions without Pooling\n",
    "        # x        : input into the block\n",
    "        # n_filters: number of filters\n",
    "        conv = self.Conv_1D_Block(inputs, num_filters, 3, 2)\n",
    "        conv = self.Conv_1D_Block(conv, num_filters, 3, 1)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def residual_block(self,inputs, num_filters):\n",
    "        # Construct a Residual Block of Convolutions\n",
    "        # x        : input into the block\n",
    "        # n_filters: number of filters\n",
    "        shortcut = inputs\n",
    "        conv = self.Conv_1D_Block(inputs, num_filters, 3, 1)\n",
    "        conv = self.Conv_1D_Block(conv, num_filters, 3, 1)\n",
    "        conv = tf.keras.layers.Add()([conv, shortcut])\n",
    "        out = tf.keras.layers.Activation('relu')(conv)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def residual_group(self,inputs, num_filters, n_blocks, conv=True):\n",
    "        # x        : input to the group\n",
    "        # n_filters: number of filters\n",
    "        # n_blocks : number of blocks in the group\n",
    "        # conv     : flag to include the convolution block connector\n",
    "        out = inputs\n",
    "        for i in range(n_blocks):\n",
    "            out = self.residual_block(out, num_filters)\n",
    "\n",
    "        # Double the size of filters and reduce feature maps by 75% (strides=2, 2) to fit the next Residual Group\n",
    "        if conv:\n",
    "            out = self.conv_block(out, num_filters * 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def learner18(self,inputs, num_filters):\n",
    "        # Construct the Learner\n",
    "        block1 = self.residual_group(inputs, num_filters, 2)          # First Residual Block Group of 64 filters\n",
    "        block2 = self.residual_group(block1, num_filters * 2, 1)           # Second Residual Block Group of 128 filters\n",
    "        block3 = self.residual_group(block2, num_filters * 4, 1)           # Third Residual Block Group of 256 filters\n",
    "\n",
    "        return block3\n",
    "\n",
    "    def stem(self,inputs, num_filters):\n",
    "        # Construct the Stem Convolution Group\n",
    "        # inputs : input vector\n",
    "        # First Convolutional layer, where pooled feature maps will be reduced by 75%\n",
    "        # In the input layer, the kernel size is 7 and strides is 2, so the 354 reduced to 177 after this filter\n",
    "        # Maxpooling. pool size = 2 means it will choose 2 element, and move by 2\n",
    "        conv =self.Conv_1D_Block(inputs, num_filters, 7, 2)\n",
    "        if conv.shape[1] <= 2:\n",
    "            print('1')\n",
    "            pool = tf.keras.layers.MaxPooling1D(pool_size=1, strides=2, padding=\"valid\")(conv)\n",
    "        else:\n",
    "            pool = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding=\"valid\")(conv)\n",
    "        return pool\n",
    "\n",
    "    def task(self, x):\n",
    "        # 63Hz frequency band branch\n",
    "        main_63 = self.residual_group(x, self.num_filters*8 , 1, False)  # Fourth Residual Block Group of 512 filters\n",
    "        main_63 = tf.keras.layers.GlobalAveragePooling1D()(main_63)\n",
    "        main_63 = tf.keras.layers.Dense(1000, activation='relu')(main_63)\n",
    "        \n",
    "        # 125Hz frequency band branch\n",
    "        main_125 = self.residual_group(x, self.num_filters*8 , 1, False)  # Fourth Residual Block Group of 512 filters\n",
    "        main_125 = tf.keras.layers.GlobalAveragePooling1D()(main_125)\n",
    "        main_125 = tf.keras.layers.Dense(1000, activation='relu')(main_125)\n",
    "        \n",
    "        # 250Hz frequency band branch\n",
    "        main_250 = self.residual_group(x, self.num_filters*8 , 1, False)  # Fourth Residual Block Group of 512 filters\n",
    "        main_250 = tf.keras.layers.GlobalAveragePooling1D()(main_250)\n",
    "        main_250 = tf.keras.layers.Dense(1000, activation='relu')(main_250) \n",
    "        \n",
    "        # 500Hz frequency band branch\n",
    "        main_500 = self.residual_group(x, self.num_filters*8 , 1, False)  # Fourth Residual Block Group of 512 filters\n",
    "        main_500 = tf.keras.layers.GlobalAveragePooling1D()(main_500)\n",
    "        main_500 = tf.keras.layers.Dense(1000, activation='relu')(main_500) \n",
    "        \n",
    "        # Surface 1 branch\n",
    "        ac1_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac1_63_output')(main_63)\n",
    "        ac1_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac1_125_output')(main_125)\n",
    "        ac1_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac1_250_output')(main_250)\n",
    "        ac1_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac1_500_output')(main_500)\n",
    "        \n",
    "        # Surface 2 branch\n",
    "        ac2_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac2_63_output')(main_63)\n",
    "        ac2_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac2_125_output')(main_125)\n",
    "        ac2_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac2_250_output')(main_250)     \n",
    "        ac2_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac2_500_output')(main_500)     \n",
    "\n",
    "        # Surface 3 branch\n",
    "        ac3_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac3_63_output')(main_63)\n",
    "        ac3_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac3_125_output')(main_125)\n",
    "        ac3_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac3_250_output')(main_250)\n",
    "        ac3_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac3_500_output')(main_500)\n",
    "\n",
    "        # Surface 4 branch\n",
    "        ac4_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac4_63_output')(main_63)\n",
    "        ac4_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac4_125_output')(main_125)\n",
    "        ac4_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac4_250_output')(main_250)\n",
    "        ac4_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac4_500_output')(main_500)\n",
    "\n",
    "        # Surface 5 branch\n",
    "        ac5_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac5_63_output')(main_63)\n",
    "        ac5_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac5_125_output')(main_125)\n",
    "        ac5_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac5_250_output')(main_250)\n",
    "        ac5_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac5_500_output')(main_500)\n",
    "\n",
    "        # Surface 6 branch\n",
    "        ac6_63_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac6_63_output')(main_63)\n",
    "        ac6_125_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac6_125_output')(main_125)\n",
    "        ac6_250_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac6_250_output')(main_250)\n",
    "        ac6_500_branch = tf.keras.layers.Dense(1, activation='sigmoid', name='ac6_500_output')(main_500)\n",
    "        \n",
    "        # Weather to include room size branch\n",
    "        if self.room_size:\n",
    "            room_size_branch = tf.keras.layers.Dense(3, activation='relu', name='roomsize_output')(main_63)\n",
    "            outputs = [ac1_63_branch,ac2_63_branch,ac3_63_branch,ac4_63_branch,ac5_63_branch,ac6_63_branch,ac1_125_branch,ac2_125_branch,ac3_125_branch,ac4_125_branch,ac5_125_branch,ac6_125_branch,ac1_250_branch,ac2_250_branch,ac3_250_branch,ac4_250_branch,ac5_250_branch,ac6_250_branch,ac1_500_branch,ac2_500_branch,ac3_500_branch,ac4_500_branch,ac5_500_branch,ac6_500_branch,room_size_branch]\n",
    "        else:\n",
    "            outputs = [ac1_63_branch,ac2_63_branch,ac3_63_branch,ac4_63_branch,ac5_63_branch,ac6_63_branch,ac1_125_branch,ac2_125_branch,ac3_125_branch,ac4_125_branch,ac5_125_branch,ac6_125_branch,ac1_250_branch,ac2_250_branch,ac3_250_branch,ac4_250_branch,ac5_250_branch,ac6_250_branch,ac1_500_branch,ac2_500_branch,ac3_500_branch,ac4_500_branch,ac5_500_branch,ac6_500_branch]\n",
    "        return outputs\n",
    "\n",
    "    def ResNet18(self):\n",
    "        # Model sequence\n",
    "        inputs = tf.keras.Input((self.length, self.num_channel), name='main_input')      # The input tensor\n",
    "        stem_ = self.stem(inputs, self.num_filters)               # The Stem Convolution Group\n",
    "        x = self.learner18(stem_, self.num_filters)               # The learner\n",
    "        outputs = self.task(x)\n",
    "        \n",
    "        # Instantiate the Model\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d3330",
   "metadata": {},
   "source": [
    "# Load data, instantiate neural network and define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07038a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "path = 'dataset/'  # dataset path\n",
    "ratio = 0.8        # ratio between training and testing data set\n",
    "# Instantiate dataset\n",
    "data = DataLoader(path,ratio)\n",
    "x_train, x_test,y_train_ac1_63, y_train_ac1_125, y_train_ac1_250, y_train_ac1_500,\\\n",
    "                y_train_ac2_63, y_train_ac2_125, y_train_ac2_250, y_train_ac2_500,\\\n",
    "                    y_train_ac3_63, y_train_ac3_125, y_train_ac3_250, y_train_ac3_500,\\\n",
    "                y_train_ac4_63, y_train_ac4_125, y_train_ac4_250, y_train_ac4_500,\\\n",
    "                    y_train_ac5_63, y_train_ac5_125, y_train_ac5_250, y_train_ac5_500,\\\n",
    "                y_train_ac6_63, y_train_ac6_125, y_train_ac6_250, y_train_ac6_500,\\\n",
    "                    y_test_ac1_63, y_test_ac1_125, y_test_ac1_250, y_test_ac1_500,\\\n",
    "                y_test_ac2_63, y_test_ac2_125, y_test_ac2_250, y_test_ac2_500,\\\n",
    "                    y_test_ac3_63, y_test_ac3_125, y_test_ac3_250, y_test_ac3_500,\\\n",
    "                    y_test_ac4_63, y_test_ac4_125, y_test_ac4_250, y_test_ac4_500,\\\n",
    "                    y_test_ac5_63, y_test_ac5_125, y_test_ac5_250, y_test_ac5_500,\\\n",
    "                    y_test_ac6_63, y_test_ac6_125, y_test_ac6_250, y_test_ac6_500,y_train_roomsize, y_test_roomsize=data.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# 2. Instaitiate the network\n",
    "length = x_train.shape[1]   # Number of Features (or length of the signal)\n",
    "num_filters = 4           # Number of Filter  in the Input Layer\n",
    "num_channel = 1             # Number of Input Channels\n",
    "room_size = True           # weather to add room size estimation\n",
    "model = ResNet(length, num_channel,num_filters,room_size).ResNet18() # Build Model\n",
    "\n",
    "# 3. Define optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "loss={'ac1_63_output': 'mse', 'ac1_125_output': 'mse', 'ac1_250_output': 'mse',\n",
    "                                     'ac1_500_output': 'mse',\n",
    "                                     'ac2_63_output': 'mse', 'ac2_125_output': 'mse', 'ac2_250_output': 'mse',\n",
    "                                     'ac2_500_output': 'mse',\n",
    "                                     'ac3_63_output': 'mse', 'ac3_125_output': 'mse', 'ac3_250_output': 'mse',\n",
    "                                     'ac3_500_output': 'mse',\n",
    "                                     'ac4_63_output': 'mse', 'ac4_125_output': 'mse', 'ac4_250_output': 'mse',\n",
    "                                     'ac4_500_output': 'mse',\n",
    "                                     'ac5_63_output': 'mse', 'ac5_125_output': 'mse', 'ac5_250_output': 'mse',\n",
    "                                     'ac5_500_output': 'mse',\n",
    "                                     'ac6_63_output': 'mse', 'ac6_125_output': 'mse', 'ac6_250_output': 'mse',\n",
    "                                     'ac6_500_output': 'mse',\n",
    "                                     'roomsize_output': 'mse'},\n",
    "                               loss_weights={'ac1_63_output': 1, 'ac1_125_output': 1, 'ac1_250_output': 1,\n",
    "                                             'ac1_500_output': 1,\n",
    "                                             'ac2_63_output': 1, 'ac2_125_output': 1, 'ac2_250_output': 1,\n",
    "                                             'ac2_500_output': 1,\n",
    "                                             'ac3_63_output': 1, 'ac3_125_output': 1, 'ac3_250_output': 1,\n",
    "                                             'ac3_500_output': 1,\n",
    "                                             'ac4_63_output': 1, 'ac4_125_output': 1, 'ac4_250_output': 1,\n",
    "                                             'ac4_500_output': 1,\n",
    "                                             'ac5_63_output': 1, 'ac5_125_output': 1, 'ac5_250_output': 1,\n",
    "                                             'ac5_500_output': 1,\n",
    "                                             'ac6_63_output': 1, 'ac6_125_output': 1, 'ac6_250_output': 1,\n",
    "                                             'ac6_500_output': 1,'roomsize_output': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f12e3",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc946405",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress = PlotProgress(entity='loss')\n",
    "Epoch = 200\n",
    "BatchSize = 64\n",
    "\n",
    "try:\n",
    "    history = model.fit({'main_input': x_train},\n",
    "                                     {'ac1_63_output': y_train_ac1_63, 'ac1_125_output': y_train_ac1_125,\n",
    "                                      'ac1_250_output': y_train_ac1_250, 'ac1_500_output': y_train_ac1_500,\n",
    "                                      'ac2_63_output': y_train_ac2_63, 'ac2_125_output': y_train_ac2_125,\n",
    "                                      'ac2_250_output': y_train_ac2_250, 'ac2_500_output': y_train_ac2_500,\n",
    "                                      'ac3_63_output': y_train_ac3_63, 'ac3_125_output': y_train_ac3_125,\n",
    "                                      'ac3_250_output': y_train_ac3_250, 'ac3_500_output': y_train_ac3_500,\n",
    "                                      'ac4_63_output': y_train_ac4_63, 'ac4_125_output': y_train_ac4_125,\n",
    "                                      'ac4_250_output': y_train_ac4_250, 'ac4_500_output': y_train_ac4_500,\n",
    "                                      'ac5_63_output': y_train_ac5_63, 'ac5_125_output': y_train_ac5_125,\n",
    "                                      'ac5_250_output': y_train_ac5_250, 'ac5_500_output': y_train_ac5_500,\n",
    "                                      'ac6_63_output': y_train_ac6_63, 'ac6_125_output': y_train_ac6_125,\n",
    "                                      'ac6_250_output': y_train_ac6_250, 'ac6_500_output': y_train_ac6_500,\n",
    "                                      'roomsize_output': y_train_roomsize},\n",
    "              epochs=Epoch, batch_size=BatchSize,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              callbacks=[plot_progress],\n",
    "              validation_split=0.2,\n",
    "             )\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636adca",
   "metadata": {},
   "source": [
    "## Save training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087007d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Item = ['Roomsize', '1-63', '1-125', '1-250', '1-500', '2-63', '2-125', '2-250', '2-500', '3-63', '3-125',\n",
    "                 '3-250', '3-500', '4-63',\n",
    "                 '4-125', '4-250', '4-500', '5-63', '5-125', '5-250', '5-500', '6-63', '6-125', '6-250', '6-500']\n",
    "EpochNumber = []\n",
    "for i in range(Epoch):\n",
    "    EpochNumber.append(str(i+1))\n",
    "\n",
    "# Save trainning loss\n",
    "TrainList = [history.history['roomsize_output_loss'], history.history['ac1_63_output_loss'],\n",
    "             history.history['ac1_125_output_loss'], history.history['ac1_250_output_loss'],\n",
    "             history.history['ac1_500_output_loss'],\n",
    "             history.history['ac2_63_output_loss'], history.history['ac2_125_output_loss'],\n",
    "             history.history['ac2_250_output_loss'], history.history['ac2_500_output_loss'],\n",
    "             history.history['ac3_63_output_loss'],\n",
    "             history.history['ac3_125_output_loss'], history.history['ac3_250_output_loss'],\n",
    "             history.history['ac3_500_output_loss'],\n",
    "             history.history['ac4_63_output_loss'], history.history['ac4_125_output_loss'],\n",
    "             history.history['ac4_250_output_loss'], history.history['ac4_500_output_loss'],\n",
    "             history.history['ac5_63_output_loss'],\n",
    "             history.history['ac5_125_output_loss'], history.history['ac5_250_output_loss'],\n",
    "             history.history['ac5_500_output_loss'],\n",
    "             history.history['ac6_63_output_loss'], history.history['ac6_125_output_loss'],\n",
    "             history.history['ac6_250_output_loss'], history.history['ac6_500_output_loss']]\n",
    "TrainLoss = pd.DataFrame(columns=EpochNumber, index=Item, data=TrainList)\n",
    "TrainLoss.to_csv('TrainLosslog.csv', encoding='utf-8')\n",
    "\n",
    "# Save testing loss\n",
    "ValList = [history.history['val_roomsize_output_loss'], history.history['val_ac1_63_output_loss'],\n",
    "            history.history['val_ac1_125_output_loss'], history.history['val_ac1_250_output_loss'],\n",
    "            history.history['val_ac1_500_output_loss'],\n",
    "            history.history['val_ac2_63_output_loss'], history.history['val_ac2_125_output_loss'],\n",
    "            history.history['val_ac2_250_output_loss'], history.history['val_ac2_500_output_loss'],\n",
    "            history.history['val_ac3_63_output_loss'],\n",
    "            history.history['val_ac3_125_output_loss'], history.history['val_ac3_250_output_loss'],\n",
    "            history.history['val_ac3_500_output_loss'],\n",
    "            history.history['val_ac4_63_output_loss'], history.history['val_ac4_125_output_loss'],\n",
    "            history.history['val_ac4_250_output_loss'], history.history['val_ac4_500_output_loss'],\n",
    "            history.history['val_ac5_63_output_loss'],\n",
    "            history.history['val_ac5_125_output_loss'], history.history['val_ac5_250_output_loss'],\n",
    "            history.history['val_ac5_500_output_loss'],\n",
    "            history.history['val_ac6_63_output_loss'], history.history['val_ac6_125_output_loss'],\n",
    "            history.history['val_ac6_250_output_loss'], history.history['val_ac6_500_output_loss']]\n",
    "ValLoss = pd.DataFrame(columns=EpochNumber, index=Item, data=TestList);\n",
    "ValLoss.to_csv('ValLosslog.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51925ac4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaa1b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate model on test data\")\n",
    "results =model.evaluate(x_test,\n",
    "[y_test_ac1_63, y_test_ac2_63, y_test_ac3_63, y_test_ac4_63, y_test_ac5_63,\n",
    "y_test_ac6_63,\n",
    "y_test_ac1_125, y_test_ac2_125, y_test_ac3_125, y_test_ac4_125,\n",
    "y_test_ac5_125, y_test_ac6_125,\n",
    "y_test_ac1_250, y_test_ac2_250, y_test_ac3_250, y_test_ac4_250,\n",
    "y_test_ac5_250, y_test_ac6_250,\n",
    "y_test_ac1_500, y_test_ac2_500, y_test_ac3_500, y_test_ac4_500,\n",
    "y_test_ac5_500, y_test_ac6_500, y_test_roomsize], batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3407a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossTerm=['OverallLoss', '1-63', '2-63', '3-63', '4-63', '5-63', '6-63', '1-125', '2-125', '3-125', '4-125',\n",
    "                 '5-125', '6-125', '1-250', '2-250', '3-250', '4-250', '5-250', '6-250', '1-500', '2-500', '3-500',\n",
    "                 '4-500', '5-500', '6-500', 'Roomsize']\n",
    "EvaluateLoss=pd.DataFrame(index=LossTerm,data=results)\n",
    "EvaluateLoss.to_csv('EvaluateLoss.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda646e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
